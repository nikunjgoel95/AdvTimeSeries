---
title: "Chapter 3"
author: "Nikunj Goel"
date: "9/26/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Loading Libraries

```{r}
library(astsa)
```

## 3.1

### Part a)

```{r}
trend = time(jj) - 1970
q = factor(cycle(jj))
req = lm(log(jj) ~ 0 + trend + q, na.action = NULL)
model.matrix(req)
summary(req)
```


### Part b)

The estimated average annual increase in the logged earnings per share is 
$$
trend
$$
which can be extracted from the summary table 

```{r}
0.167
```

### Part c)

Average logged earning rate $q4$ - $q3$ decreases.
```{r}
0.882266 - 1.151024
```

The percentage of the decrease is 
```{r}
(0.269/1.151024) * 100
```


### Part d)

```{r}
trend = time(jj) - 1970
q = factor(cycle(jj))
req = lm(log(jj) ~ trend + q, na.action = NULL)
summary(req)
```

Having an intercept removes the first quarter away.
Also the intercept appears in all the quarters. It is of no use because we want to analyze all the quarters separately.

### Part e)

```{r}
par(mfrow=c(1,2))
plot(log(jj), main="plot of data and fitted value")
lines(fitted(req), col="red") 
plot(log(jj) - fitted(req), main="plot of residuals")
```


The residuals are not following any pattern hence it looks fairly white.
The fit of the model is decent.

## 3.2

### Part a)

```{r}
temp = tempr - mean(tempr)
temp2 = temp^2
trend = time(cmort)
partL4 = lag(part, -4)

ded = ts.intersect(cmort, trend, temp, temp2, part, partL4)
fit = lm(cmort ~ trend + temp + temp2 + part + partL4, data = ded, na.action = NULL)
summary(fit)
```

partL4 has a good significant and model has a good fit.

### Part b)

```{r}

AIC(fit)/nrow(ded) - log(2*pi)
BIC(fit)/nrow(ded) - log(2*pi)
```

since the AIC and BIC are lower, its a better model fit than before with the new explantory variable

## 3.4

### Part a)

The equation is 

$$
x_{t} = \beta_{0} + \beta_{1}t + w_{t}
$$
Mean of $x_{t}$

$$
E[x_{t}] = E[\beta_{0} + \beta_{1}t + w_{t}] = \beta_{0} + \beta_{1}t
$$

It is dependent on time t, hence the series $x_{t}$ is not stationary.

### Part b)

The first order difference can be simplified into

$$
\Delta x_{t} = x_{t} - x_{t-1}
\\
= \beta_{0} + \beta_{1}t + w_{t} - [\beta_{0} + \beta_{1}(t-1) + w_{t-1}]
\\
= w_{t} - w_{t-1} + \beta_{1}
$$
Mean of the function would be

$$
E[\Delta x_{t}] = E[w_{t} - w_{t-1} + \beta_{1}]
\\
=\beta_{1} + E[w_{t}] - E[w_{t-1}]
\\
=\beta_{1}
$$

Independent of time

and now Auto Covariance Function is:

$$
\gamma \Delta x_{t}(t+h,h) = Cov(x_{t+h},x_{t})
\\
= Cov(w_{t+h} - w_{t+h-1} + \beta_{1},w_{t} - w_{t-1} + \beta_{1})
\\
= Cov(w_{t+h} - w_{t+h-1},w_{t} - w_{t-1})
\\
\begin{cases}
2 \sigma^2_{w}, & \text{h=0}
\\
-\sigma^2_{w}, & \text{|h| = 1}
\\
0, & \text{|h|>1}
\end{cases}
$$
which is also free from t, hence $\Delta x_{t}$ is stationary.

### Part c)

$$
\gamma \Delta x_{t}(t+h,h) = Cov(x_{t+h},x_{t})
\\
=Cov(\beta_{1} + y_{t+h} - y_{t+h-1},\beta_{1} + y_{t} - y_{t-1})
\\
=Cov(y_{t+h} - y_{t+h-1},y_{t} - y_{t-1})
\\
=Cov(y_{t+h},y_{t}) - Cov(y_{t+h},y_{t-1}) -Cov(y_{t+h-1},y_{t}) + Cov(y_{t+h-1},y_{t-1})
\\
=-\gamma_{y}(h+1) + 2\gamma_{y}(h) - \gamma_{y}(h-1)
$$
which is independent of time since $y_{t}$ is stationary and $\gamma_{t}$(·)is independent of time t.
So it is Stationary.

## 3.6

### Part a)

```{r}
n = length(varve)
varve1 = varve[1:n/2]
varve2 = varve[(n/2+1):n]
data.frame(firstHalf = var(varve1),SecondHalf=var(varve2))
```

as we can see the sample variance in the second half of the data is significanlty larger than the first half,therefore the variance is not homogenous, hence we need to statibilize the variance by data transformation such as log(·)-transformation. Having a log transformation obviously made data closer to the normality.

```{r}
par(mfrow= c(1,2))
hist(varve, main="raw data")
hist(log(varve), main="log-transformed data")
```

### Part b)

```{r}
plot(log(varve), main="log-transformed data")
```
It doesn't that there is an comparable trend over time as compared to gtemp.

```{r}
ts.plot(gtemp)
```

### part c)

```{r}
acf1(log(varve))
```
that the dependency in the data is very strong in close lags and dies down very slowly.

### Part d)

```{r}
u = diff(log(varve), 1)
plot(u)
acf1(u)
```


Differencing produces fairly reasonable stationary process. $u_{t}$can be interpreted as the yearly increase in the thicknesses varves.

In Statistical sense it can also be smoothing.

$$
u_{t} = \Delta y_{t}
\\
=y_{t} - y_{t-1}
\\
=log(x_{t}) - log(x_{t-1})
\\
=log(\frac{x_{t}}{x_{t-1}})
\\
=log(1 + \frac{x_{t} - x_{t-1}}{x_{t-1}})
\\
\approx \frac{x_{t} - x_{t-1}}{x_{t-1}}
$$
which can be intepreted as the marginal change in the thickness varves.

## 3.7


**MA Smoothing**
```{r}
#MA Smoothing
culer = c(rgb(.85,.30,.12,.6), rgb(.12,.65,.85,.6))
w = c(.5, rep(1,11), .5)/12
gtemp_land_f = filter(gtemp_land, sides=2, filter=w)
tsplot(gtemp_land, col=culer[1], ylim=c(-1, 1.65))
lines(gtemp_land_f, lwd=2, col=2)
gtemp_ocean_f = filter(gtemp_ocean, sides = 2, filter = w)
lines(gtemp_ocean, col=culer[2])
lines(gtemp_ocean_f, lwd=2, col=4)
```

**Kernel Smoothing**
```{r}
culer = c(rgb(.85,.30,.12,.6), rgb(.12,.65,.85,.6))
w = c(.5, rep(1,11), .5)/12
gtemp_land_f = ksmooth(time(gtemp_land),gtemp_land,"normal", bandwidth = 12)
tsplot(gtemp_land, col=culer[1], ylim=c(-1, 1.65))
lines(gtemp_land_f, lwd=2, col=2)
gtemp_ocean_f = ksmooth(time(gtemp_ocean),gtemp_ocean,"normal", bandwidth = 12)
lines(gtemp_ocean, col=culer[2])
lines(gtemp_ocean_f, lwd=2, col=4)
```
**Lowess Smoothing**
```{r}
culer = c(rgb(.85,.30,.12,.6), rgb(.12,.65,.85,.6))
w = c(.5, rep(1,11), .5)/12
gtemp_land_f = lowess(gtemp_land)
tsplot(gtemp_land, col=culer[1], ylim=c(-1, 1.65))
lines(gtemp_land_f, lwd=2, col=2)
gtemp_ocean_f = lowess(gtemp_ocean)
lines(gtemp_ocean, col=culer[2])
lines(gtemp_ocean_f, lwd=2, col=4)
```

All methods seems to model the trend reasonably, but one needs to be careful on picking the tunning parameters such as the weights of MA smoothing, bandwitdh of the kernel smoothing, and so on.
MA starts 3 years early and late. Kernel smoothing gives us the smoothest trend of the 3. Lowess is more localised.

## 3.9

```{r}
culer = c("cyan4", 4, 2, 6)
par(mfrow = c(4, 1), cex.main = 1)
x = window(log(jj), start = 1960)
out = stl(x, s.window = "periodic")$time.series

tsplot(x, main = "Johnson & Johnson", ylab = "", col = gray(0.7))
text(x, labels = 1:4, col = culer, cex = 1.25)
tsplot(out[,1], main = "Seasonal", ylab = "", col = gray(0.7))
text(out[,1], labels = 1:4, col = culer, cex = 1.25)
tsplot(out[,2], main = "Trend", ylab = "", col = gray(0.7))
text(out[,2], labels = 1:4, col = culer, cex = 1.25)
tsplot(out[,3], main = "Noise", ylab = "", col = gray(0.7))
text(out[,3], labels = 1:4, col = culer, cex = 1.25)
```
```{r}
culer = c("cyan4", 4, 2, 6)
par(mfrow = c(4, 1), cex.main = 1)
x = window(log(jj), start = 1960)
plot(decompose(x))
```

The Seasonal plot shows an increase from Q1 to Q2, increase from Q2 to Q3, sharp decrease from Q3 to 4, and an increase back from Q4 to the next Q1. The Trend plot shows a steady increase over the 20 years of the data from quarter to quarter, relatively smooth. The Noise plot shows higher deviations from Q3 and Q4 from the data than Q1 and Q2.

 

In 3.1 we had a good model fit (residuals around 0 and a trend line that fit the data well). Here, we see a Noise plot that would indicate a good fit, all around 0, and a seasonal trend that is somewhat regular (in terms of direction but not always in magnitude). This generally agrees with the model in 3.1 being a good fit.