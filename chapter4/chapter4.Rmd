---
title: "chapter4"
author: "Nikunj Goel"
date: "10/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 4.2

### Part a)

Using **Mathematical Induction** to prove the formula
$$
x_{t} = \sum_{j=0}^{t}\phi^jw_{t-j}
\\
x_{0} = w_{0}
\\
x_{1} = \phi^0w_{1} + \phi^1w_{0}
\\
x_{2} = \phi^0w_{2} + \phi^1w_{1} +\phi^2w_{0}
\\
x_{3} = \phi^0w_{3} + \phi^1w_{2} +\phi^2w_{1} + \phi^3w_{0}
\\
Assuming \ it \ to \ be \ true \ for \ x_{t-1}
\\
The \ expression \ hold \ for \ x_{t} 
\\
x_{t} = \phi(\sum_{k=0}^t \phi^kw_{t-1-k}) + w_{t}
\\
Hence \ Proved
$$

### Part b)

$$
Right \  hand \ side \ is
\\
= w_{t} + \phi w_{t-1}[k=0] + \phi^2w_{t-2}[k=1] + ...
\\
E(x_{t}) = \sum_{j=0}^t\phi^jE(w_{t-j}) = 0
$$
 
### Part c)
 
$$
var(x_{t}) = cov(x_{t},x_{t})
\\
= cov(\sum_{j=0}^{t} \phi^jw_{t-j},\sum_{k=0}^t\phi^kw_{t-k})
\\
= \sigma^2_{w} \sum_{j=0}^t \phi^j \phi^j
\\
= \sigma^2_{w} \sum_{j=0}^t \phi^{2j}
\\
Using \ the \ expansion
\\
\sum_{j=0}^{k} a^j = \frac{(1 - a^{k+1})}{(1-a)} 
\\
= \sigma^2_{w} \frac{(1-\phi^{2(t+1)})}{1 - \phi^2}
$$
 
 
### Part d)

$$
cov(x_{t+h},x_{t}) = cov(\sum_{j=0}^{t} \phi^jw_{t+h-j},\sum_{k=0}^t\phi^kw_{t-k})
\\
= cov[w_{t+h} +....+ \phi^hw_{t} +.., \phi^0w_{t} + \phi w_{t-1}+...]
\\
= \sigma^2_{w} \sum_{j=0}^t \phi^{j+h} \phi^j
\\
= \phi^h \sigma^2_{w} \sum_{j=0}^t \phi^{j} \phi^j
\\
= \phi^h \sigma^2_{w} \sum_{j=0}^t \phi^{2j}
\\
\\
Using \ the \ expansion
\\
\sum_{j=0}^{k} a^j = \frac{(1 - a^{k+1})}{(1-a)} 
\\
= \phi^h var(x_{t})
$$

### Part e)

$x_{t}$ process is not stationary as Co-variance is dependent of time.

### Part f)

When $t -> \infty$ , process is stationary because $|\phi| < 1$
$$
var(x_{t}) = \frac{\sigma^2_{w}}{1 - \phi^2}
$$
As now variance is independent of time. 
Here as the time is increasing the series is approaching to stationarity so $x_t$ is called 'asymptotically stationary'.

### Part g)

If we generate n observations we basically get rid of the beginning, like its warming up, and then it turns into gaussian ar(1) because phi < 1 which is iid N(0,1)

### Part h)

Now $x_0 = \frac{w_0}{\sqrt{1-\phi^2}}$

$$
\begin{aligned}
x_t &= \phi x_{t-1}+w_t
\\
for \ \ t = 1
\\
x_1 &= \phi x_0 + w_1
\\
&= \phi \frac{w_0}{\sqrt{1-\phi^2}} + \phi^0w_1
\\
for \ \ t = 2
\\
x_2 &= \phi x_1 + w_2
\\
&= \phi(\phi \frac{w_0}{\sqrt{1-\phi^2}} + \phi^0w_1) + \phi^0w_2
\\
&= \phi^2 \frac{w_0}{\sqrt{1-\phi^2}} + \phi w_1 + \phi^0w_2
\\
for \ \ t = 3
\\
x_3 &= \phi x_2 + w_3
\\
&= \phi(\phi^2 \frac{w_0}{\sqrt{1-\phi^2}} + \phi w_1 + \phi^0w_2) + w_3
\\
&= \phi^3 \frac{w_0}{\sqrt{1-\phi^2}} + \phi^2 w_1 + \phi w_2 + \phi^0w_3
\\
&. \\
&. \\
&. \\
&. \\
Using \ mathematical \ &induction \  we \ can \ write \ below \ for \ x \ at \ t
\\
x_t &= \frac{\phi^t w_0}{\sqrt{1-\phi^2}}+ \sum_{j=0}^{t-1} \phi^jw_{t-j}
\end{aligned}
$$

The mean of $x_t$ is
$$
\begin{aligned}
E[x_t] &= E[\frac{\phi^t w_0}{\sqrt{1-\phi^2}}+ \sum_{j=0}^{t-1} \phi^jw_{t-j}]
\\
&= \frac{\phi^t}{\sqrt{1-\phi^2}} E[w_0] + \sum_{j=0}^{t-1} \phi^jE[w_{t-j}]
\\
&= 0 \ \ \ as \ \ \ E[w_t] = 0 \ \ \ for \ \ all \ \ t.
\end{aligned}
$$

From the above we can define the variance of $x_{t}$ as

$$
\begin{aligned}
var(x_t) &= cov(\frac{\phi^t w_0}{\sqrt{1-\phi^2}}+ \sum_{j=0}^{t-1} \phi^jw_{t-j}, \frac{\phi^t w_0}{\sqrt{1-\phi^2}}+ \sum_{k=0}^{t-1} \phi^kw_{t-k})
\\
&= cov(\frac{\phi^t w_0}{\sqrt{1-\phi^2}},\frac{\phi^t w_0}{\sqrt{1-\phi^2}}) + cov(\frac{\phi^t w_0}{\sqrt{1-\phi^2}},\sum_{j=0}^{t-1} \phi^jw_{t-j}) + cov(\frac{\phi^t w_0}{\sqrt{1-\phi^2}},\sum_{k=0}^{t-1} \phi^kw_{t-k})
\\
& \ \ \ \ \ \ + cov(\sum_{j=0}^{t-1} \phi^jw_{t-j}, \sum_{k=0}^{t-1} \phi^kw_{t-k})
\\
&= \frac{\phi^{2t} \sigma_w^2}{1-\phi^2} + 0 + 0 + cov(\sum_{j=0}^{t-1} \phi^jw_{t-j}, \sum_{k=0}^{t-1} \phi^kw_{t-k})
\\
&= \frac{\phi^{2t} \sigma_w^2}{1-\phi^2} + \sigma_w^2 \sum_{j=0}^{t-1}\phi^{2j}
\\
&= \frac{\phi^{2t} \sigma_w^2}{1-\phi^2} + \sigma_w^2 \frac{(1-(\phi^2)^{(t-1+1)})}{(1-\phi^2)}
\\
&= \frac{\phi^{2t} \sigma_w^2}{1-\phi^2} + \sigma_w^2\frac{(1-\phi^{2t})}{(1-\phi^2)}
\\
&= \frac{\sigma_w^2}{1-\phi^2} (\phi^{2t} + 1 - \phi^{2t})
\\
var(x_t) &= \frac{\sigma^2}{1-\phi^2}
\end{aligned}
$$
From the above the mean of $x_t$ is constant and covariance is not dependent on time so the process is stationary.

## Problem 4.3

Given the two models are $x_t = .80x_{t-1}-.15x_{t-2}+w_t-.30w_{t-1}$ and $x_t = x_{t-1} - .50x_{t-2}+w_t-w_{t-1}$

### Part a)

Model 1
$$
\begin{aligned}
x_t &= .80x_{t-1}-.15x_{t-2}+w_t-.30w_{t-1}
\\
(0.15B^2 - 0.8B + 1) x_t &= (1 - 0.3B) w_t
\\
(0.15B^2 - 0.5B - 0.3B + 1)x_t &= (1-0.3B)w_t
\\
(0.5B(0.3B - 1) - 1(0.3B - 1))x_t &= (1-0.3B)w_t
\\
(0.5B-1)(0.3B-1)x_t &= (1 - 0.3B)w_t
\\
Reduced \ \ form \ \ below
\\
(0.5B - 1)x_t &= -w_t
\\
x_t - 0.5x_{t-1} &= w_t
\end{aligned}
$$
From the above we can cancel out (1-0.3B) on each side and model is only AR(1). The model has parameter redundancy and the reduced form of the model is $x_t = 0.5x_{t-1} + w_t$

Model 2
$$
\begin{aligned}
x_t &= x_{t-1} - .50x_{t-2}+w_t-w_{t-1}
\\
x_t - x_{t-1} + 0.5x_{t-2} &= w_t - w_{t-1}
\\
(1 - B + 0.5B^2)x_t &= (1-B)w_t
\\
The \ \ above \ \ equation \ \ has \ \ complex \ \ roots \ \ on \ \ left \ \ side
\\
0.5(B - 1 + i)(B - 1 - i) x_t &= (1-B)w_t
\end{aligned}
$$

From the above there are no common roots on both sides. So the model is already in its reduced form only.

## Part b)

Model 1
```{r}
library(astsa)

Mod(polyroot( c(1,-.5) ))

Mod(polyroot(c(1)))
```

From the above we can see that the magnitude of the root is not less than or equal to 1 for AR. So the ARMA model-1 in reduced form is causal.

It is similar to MA(1) model and it is invertible.


Model 2
```{r}
Mod(polyroot( c(1,-.1, .5) ))

Mod(polyroot( c(1,-1) ))
```

From the above we can see that the magnitude of the roots is not less than or equal to 1 for AR. So the ARMA model-2 in reduced form is causal.
Where as the roots of MA are equal to one in magnitude. So the ARMA model-2 in reduced form is not invertible.

### Part c)

```{r}
#Model 1
round(ARMAtoMA(ar = .5, ma = 0, 50), 3)
round(ARMAtoAR(ar = .5, ma = 0, 50), 3)
```

From the above Model-1 we can see that ARMAtoMA is causal as the coefficients are decreasing to zero fast . 
The ARMAtoAR is also invertible as the  coefficients converged to zero from 2nd coefficients.

```{r}
#Model 2
round(ARMAtoMA(ar = c(1, -.5), ma = -1, 50), 3)
round(ARMAtoAR(ar = c(1, -.5), ma = -1, 50 ), 3)
```

From the above Model-2 we can seen ARMAtoMA is causal as the coefficients converged to zero.
But ARMAtoAR is not invertible as the coefficients are constant and are never converging to zero.

## Problem 4.4

### Part a)

```{r}
ACF = ARMAacf(ar = c(0.6), ma = c(0.9), 24)[-1]
PACF = ARMAacf(ar = c(0.6), ma = c(0.9), 24, pacf = TRUE)
par(mfrow = 1:2)
tsplot(ACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
tsplot(PACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
```

```{r}
ACF = ARMAacf(ar = c(0.6), ma = 0, 24)[-1]
PACF = ARMAacf(ar = c(0.6), ma = 0, 24, pacf = TRUE)
par(mfrow = 1:2)
tsplot(ACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
tsplot(PACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
```

```{r}
ACF = ARMAacf(ar = 0, ma = c(0.9), 24)[- 1 ]
PACF = ARMAacf(ar = 0, ma = c(0.9), 24, pacf = TRUE)
par(mfrow = 1:2)
tsplot(ACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
tsplot(PACF, type = "h", xlab = "lag", ylim = c(-.8, 1))
abline(h = 0)
```

In ARMA(1,1) both acf & pacf tails off and it is little difficult to find the value of *p* and *q*.
In ARMA(1,0) acf tails off and pacf cutf off after lag p (p=1 in this case) which gives us the insight that the model is AR(1).
In ARMA(0,1) acf cuts off after lag q(q=1 in this case) and pacf tails off which gives us the insight that the model is MA(1).

### Part b)

```{r}
acf2(arima.sim(list(order=c(1,0,1), ar=.6, ma=.9), n=100),main = "ARMA(1,1)")
acf2(arima.sim(list(order=c(1,0,0), ar=.6), n=100),main = "ARMA(1,0)")
acf2(arima.sim(list(order=c(0,0,1), ma=.9), n=100),main = "ARMA(0,1)")
```
The practical values are close to the theoretical values but in practical values are negligible but not zero.

### Part c)

```{r}
acf2(arima.sim(list(order=c(1,0,1), ar=.6, ma=.9), n=500),main = "ARMA(1,1)")
acf2(arima.sim(list(order=c(1,0,0), ar=.6), n=500),main = "ARMA(1,0)")
acf2(arima.sim(list(order=c(0,0,1), ma=.9), n=500),main = "ARMA(0,1)")
```
As the number is higher, it more closely relates to theoretical values.

## Problem 4.5

### Part a)

```{r}
tsplot(diff(cmort))
```

Differencing seems reasonable in this case as it produces fairly stationary time series and removes the trend from the data.

### Part b)

```{r}
acf2(diff(cmort))
```

AR(1) is appropriate for $x_{t}$ because ACF is tailing off.PACF cuts off after lag = 1 and values are nearly closely to zero.

### Part c)

```{r}
sarima(diff(cmort), p=1,d=0,q=0, no.constant = TRUE)
```
As the P value is zero, it proves the regression coefficients are significant.

The estimate of the white noise variance is 0.00146

### Part d)

```{r}
library(astsa)
value = sarima(diff(cmort), p=1,d=0,q=0, no.constant = TRUE)
tsplot(resid(value$fit))
acf1(resid(value$fit))
```
If we see the acf1 plot of the residuals it clearly shows that it is white noise.


### Part e)

```{r}
forecastPlot <- sarima.for(diff(cmort), n.ahead=4,p=1,d=0,q=0,no.constant = TRUE)
```
The Predicted values
```{r}
forecastPlot$pred
```

The 95% interval values
```{r}
forecastPlot$pred + 1.96*forecastPlot$se
forecastPlot$pred - 1.96*forecastPlot$se
```
### Part f)

The forecast is done based on the equation $x_{n+m}^n = \phi^{m} x_n + w_{n+m}$. Where the $\phi^m$ values are the PACF values of lag m at. So here if we want to forecast next value we are calculating $x_{n+1}^n = \phi x_n + w_{n+1}$.

As the $w_{n+1}$ is future error we equate it to zero $w_{n+1} = 0$. We can write the m - step ahead forecast as

$$
x_{n+m}^n = \phi^m x_n
$$
The last value of xt is
```{r}
xt = diff(cmort)
xt[length(xt)]
```
From the above PACF plots from 4.5b the lag1 values are [-0.51]
PACF at lag 1 is -0.51. So we can calculate our Next value which is
$$
\begin{aligned}
x_{n+1} &= \phi x_{n}
\\
x_{n+1} &= (-0.51) * (-3.94)
\\
&= 2.0094
\end{aligned}
$$
$$
\begin{aligned}
x_{n+2} &= \phi^2 x_{n}
\\
x_{n+2} &= (-0.51)^2 * (-3.94)
\\
&= -1.024
\end{aligned}
$$


$$
\begin{aligned}
x_{n+3} &= \phi^3 x_{n}
\\
x_{n+3} &= (-0.51)^3 * (-3.94)
\\
&= 0.5226
\end{aligned}
$$

$$
\begin{aligned}
x_{n+4} &= \phi^4 x_{n}
\\
x_{n+4} &= (-0.51)^4 * (-3.94)
\\
&= -0.2665
\end{aligned}
$$

### Part g)

```{r}
acf2(cmort)
```
After seeing the acf2 plot we can easily see that pacf cuts at lag=2 which means ARMA(2,0) would be a better model for the data. 

```{r}
sarima.for(cmort, n.ahead=1,p=2,d=0,q=0)
```
